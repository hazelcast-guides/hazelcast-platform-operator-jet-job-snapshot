= Saving the State of Jet Job with Hazelcast Platform Operator
:page-layout: tutorial
:page-product: operator
:page-categories: Cloud Native
:page-lang: go, java, node, python
:page-enterprise: true
:page-est-time: 10 mins
:github-directory: https://github.com/hazelcast-guides/hazelcast-platform-operator-jet-job-snapshot
:description: Learn how to save the state of your data pipelines and initiate a data pipeline from an existing state using the Hazelcast Platform Operator.

{description}

== Context

This tutorial will guide you through the process of saving the state of your data pipelines and creating new data pipelines initialized from the saved state. We will be following the steps below during the tutorial:

- Deploy a Hazelcast cluster with the configured Jet engine.

- Initiate the Jet pipeline.

- Export a Snapshot from the Jet pipeline.

- Start a new Jet pipeline initialized from the Snapshot.

== Before you Begin

Make sure you have the following:

* Running https://kubernetes.io/[Kubernetes] cluster
* Kubernetes command-line tool, https://kubernetes.io/docs/tasks/tools/#kubectl[kubectl]
* Maven command-line tool, https://maven.apache.org/download.cgi[mvn]
* Deployed xref:operator:ROOT:index.adoc[Hazelcast Platform Operator version 5.8 or above]
* Blob storage and access credentials on one of the following cloud providers: https://aws.amazon.com/s3/[AWS], https://cloud.google.com/storage/[GCP], https://azure.microsoft.com/en-us/services/storage/blobs/[Azure]

If you're unsure about how to submit JetJobs using the Hazelcast Platform Operator, it's recommended to start by following the 'data pipeline' tutorial first. This tutorial also covers the steps for deploying a custom Jet pipeline JAR to cloud storage and provides guidance on managing JetJob resources using the Hazelcast Platform Operator.

== Step 1. Start the Hazelcast Cluster

Since Snapshot is an exclusive feature of the enterprise version, you'll need to set up an Enterprise Hazelcast cluster. This is why it's necessary to create a License Key secret first.

. Execute the following command to create a Hazelcast License Key secret, which will be referenced in the Hazelcast resource definition later:

+
[source, shell]
----
kubectl create secret generic <SECRET-NAME> --from-literal=license-key=<LICENSE-KEY>
----
+

. Run the following command to create a Hazelcast cluster with the Jet engine configured:

+
[source, shell]
----
cat <<EOF | kubectl apply -f -
apiVersion: hazelcast.com/v1alpha1
kind: Hazelcast
metadata:
  name: hazelcast
spec:
  clusterSize: 1
  repository: 'docker.io/hazelcast/hazelcast-enterprise'
  version: '5.3.0'
  licenseKeySecretName: <LICENSE-KEY-SECRET-NAME>
  jet:
    enabled: true
    resourceUploadEnabled: true
EOF
----
+

NOTE: If you want to explicitly specify the Hazelcast version, it's important to note that you should use Hazelcast Enterprise version 5.3 or above for submitting JetJobs using the Hazelcast Platform Operator.

. Now, verify the cluster's status to ensure that it is up and running.

+
[source, shell]
----
$ kubectl get hazelcast hazelcast
NAME        STATUS    MEMBERS   EXTERNAL-ADDRESSES   WAN-ADDRESSES
hazelcast   Running   1/1
----

== Step 2. Run the Data Pipeline

In the tutorial, we will walk through a practical example involving a 'transaction' map designed to store crucial transaction data. Our Hazelcast Jet setup will seamlessly retrieve these transactions from the map and process them. Firstly, we will submit a data pipeline that processes transactions from the 'transaction' map. To maintain simplicity in the example, the pipeline will only log the transactions. Then we will explore the process of saving the state of the data pipeline using the Hazelcast Platform Operator.

Before submitting the Data Pipelines, you need to create a map from which the Data Pipeline will fetch the records. To utilize the map as a data source for a Data Pipeline, it's necessary to enable the Event Journal in the map.

. Execute the following command to create the Map, which will serve as the source for the Data Pipeline

+
[source, shell]
----
cat <<EOF | kubectl apply -f -
apiVersion: hazelcast.com/v1alpha1
kind: Map
metadata:
  name: transaction
spec:
  hazelcastResourceName: hazelcast
  name: transaction
  eventJournal:
    capacity: 9000
EOF
----

NOTE: It is important to configure the event journal in the map if it is being used as a data source for a Data Pipeline.

You are all set to submit the data pipeline that processes transactions from the map. Below is the Java source code for the pipeline we've created in this tutorial. This pipeline simply retrieves transaction entries from the map and logs them for the sake of simplicity. If you are unfamiliar with submitting JetJobs with the Hazelcast Platform Operator, you can also refer to the Jet tutorial.

+
[source, java]
----
public static void main(String[] args) {
    var p = Pipeline.create();
    var transactionSource = Sources.mapJournal("transaction", JournalInitialPosition.START_FROM_OLDEST);
    var loggerSink = Sinks.logger();
    p.readFrom(transactionSource)
            .withIngestionTimestamps()
            .setName("Emit Transactions")
            .map(e -> {
                System.out.printf("The transaction '%s' is being executed in 'job-v1'\n", e.getKey());
                // execute the transaction
                return String.format("[Job V1] transaction:'%s' payload:'%s'", e.getKey(), e.getValue());
            })
            .setName("Apply Transactions")
            .writeTo(loggerSink)
            .setName("Log Transactions");

    HazelcastInstance hz = Hazelcast.bootstrappedInstance();
    hz.getJet().newJob(p);
}
----

. Execute the following command to submit the Data Pipeline.

+
[source, shell]
----
cat <<EOF | kubectl apply -f -
apiVersion: hazelcast.com/v1alpha1
kind: JetJob
metadata:
  name: job-transaction-v1
spec:
  name: transaction-v1
  hazelcastResourceName: hazelcast
  state: Running
  jarName: jet-pipelines-1.0-SNAPSHOT.jar
  mainClass: org.examples.jet.snapshot.JobV1
  bucketConfig:
    bucketURI: '<BUCKET-URI>'
    secretName: '<SECRET-NAME>'
EOF
----


. Run the following command to check the status of the JetJob you have submitted.

+
[source, shell]
----
$ kubectl get jetjob job-transaction-v1
NAME                 STATUS    ID                   SUBMISSIONTIME         COMPLETIONTIME
job-transaction-v1   Running   741632319877545985   2023-08-09T12:22:04Z
----

As new entries are added to the 'transaction' map, the data pipeline will automatically retrieve and process them. To observe the executed transactions, examine the logs. In the provided log example below, three transactions are processed with keys 'transaction-1', 'transaction-2', and 'transaction-3'. The entry values are not a concern in this context.

+
[source]
----
The transaction 'transaction-1' is being executed in 'job-v1'
{"time":"2023-08-09T12:24:59,753", "logger": "com.hazelcast.jet.impl.connector.WriteLoggerP", "level": "INFO", "msg": "[10.36.0.10]:5702 [dev] [5.3.0] [transaction-v1/Log Transactions#0] [Job V1] transaction:'transaction-1' payload:'{\"description\": \"Online Purchase\", \"amount\": 75.99, \"transactionDate\": \"2023-08-09T15:30:00Z\"}' "}
The transaction 'transaction-2' is being executed in 'job-v1'
{"time":"2023-08-09T12:33:32,784", "logger": "com.hazelcast.jet.impl.connector.WriteLoggerP", "level": "INFO", "msg": "[10.36.0.10]:5702 [dev] [5.3.0] [transaction-v1/Log Transactions#0] [Job V1] transaction:'transaction-2' payload:'{\"description\": \"Grocery Shopping\", \"amount\": 42.75, \"transactionDate\": \"2023-08-10T10:15:00Z\"}' "}
The transaction 'transaction-3' is being executed in 'job-v1'
{"time":"2023-08-09T12:33:44,997", "logger": "com.hazelcast.jet.impl.connector.WriteLoggerP", "level": "INFO", "msg": "[10.36.0.10]:5702 [dev] [5.3.0] [transaction-v1/Log Transactions#0] [Job V1] transaction:'transaction-3' payload:'{\"description\": \"Restaurant Dinner\", \"amount\": 120.50, \"transactionDate\": \"2023-08-11T20:00:00Z\"}' "}
----

== Step 3. Saving the state of Data Pipeline

In data pipelines, saving and using computation process states is vital for accurate and reliable data processing. Jet's Snapshot feature lets you save and restore these processing states. A snapshot captures the state of a running Jet job at a specific time, giving you a reliable record of ongoing computations and processed data.

. Run the following command to export a Snapshot from the Data Pipeline.

+
[source, shell]
----
cat <<EOF | kubectl apply -f -
apiVersion: hazelcast.com/v1alpha1
kind: JetJobSnapshot
metadata:
  name: snapshot-transaction
spec:
  name: transaction
  jetJobResourceName: job-transaction-v1
  cancelJob: true
EOF
----

. Use the following command to check the status of the exported JetJobSnapshot:

+
[source, shell]
----
$ kubectl get jetjobsnapshot snapshot-transaction
NAME                   STATE      CREATIONTIME
snapshot-transaction   Exported   2023-08-09T13:07:51Z
----

NOTE: By configuring the 'spec.cancelJob' field to 'true', the data pipeline named 'job-transaction-v1' will be canceled after applying the JetJobSnapshot. This setting is particularly useful before submitting a new version of the active data pipeline. With this approach, the snapshot will halt the ongoing job after preserving its current state.


. The data pipeline should not be in the Running state anymore. You can verify this by using the following command:

+
[source, shell]
----
$ kubectl get jetjob job-transaction-v1
NAME                 STATUS            ID                   SUBMISSIONTIME         COMPLETIONTIME
job-transaction-v1   ExecutionFailed   741632319877545985   2023-08-09T12:22:04Z   2023-08-09T13:07:51Z
----

== Step 4. Submit Job initialized from Snapshot

We can submit data pipeline which are initialized from a Snapshot which we exported before. In this way, we are able to continue to process data starting from where the state of the Snapshot is exported.

It is source code of the new version of Data Pipeline which we created before.

+
[source, java]
----
public static void main(String[] args) {
    var p = Pipeline.create();
    var transactionSource = Sources.mapJournal("transaction", JournalInitialPosition.START_FROM_OLDEST);
    var loggerSink = Sinks.logger();
    p.readFrom(transactionSource)
            .withIngestionTimestamps()
            .setName("Emit Transactions")
            .map(e -> {
                System.out.printf("The transaction '%s' is being executed in 'job-v2'\n", e.getKey());
                // execute the transaction
                return String.format("[Job V2] transaction:'%s' payload:'%s'", e.getKey(), e.getValue());
            })
            .setName("Apply Transactions")
            .writeTo(loggerSink)
            .setName("Log Transactions");

    HazelcastInstance hz = Hazelcast.bootstrappedInstance();
    hz.getJet().newJob(p);
}
----

. Run the command to submit Data Pipeline which is initialized from a Snapshot which we exported before.

+
[source, shell]
----
cat <<EOF | kubectl apply -f -
apiVersion: hazelcast.com/v1alpha1
kind: JetJob
metadata:
  name: job-transaction-v2
spec:
  name: transaction-v2
  hazelcastResourceName: hazelcast
  state: Running
  jarName: jet-pipelines-1.0-SNAPSHOT.jar
  mainClass: org.examples.jet.snapshot.JobV2
  initialSnapshotResourceName: snapshot-transaction
  bucketConfig:
    bucketURI: '<BUCKET-URI>'
    secretName: '<SECRET-NAME>'
EOF
----

This data pipeline will directly continue to processing the records which are put after the Snapshot. 

. You can check the status of the Data Pipeline in the same way as the previous.

+
[source, shell]
----
$ kubectl get jetjob job-transaction-v2
NAME                 STATUS    ID                   SUBMISSIONTIME         COMPLETIONTIME
job-transaction-v2   Running   741650518446702593   2023-08-09T13:34:22Z
----

You will see the logs of only the new transaction records that are put after the time when the Snapshot is exported.  

If the new version of Data Pipeline 'job-transaction-v2' is not initialized form the Snapshot 'snapshot-transaction' which we exported, It would start processing data from the beginning of the records. That is not what we want, because we have already processed the data in the previous version of Data Pipeline. 

NOTE: If want to make your Snapshots to be persistent against outages or restarts, it would be enough to create a Hazelcast cluster with persistence enabled.

== Summary

Saving the current state of your data pipeline and initializing new pipelines from that snapshot could be essential in same cases as demonstrated the example in the tutorial. We have covered the process of managing the state of your Data Pipelines using the Hazelcast Platform Operator.

== See Also

- xref:operator:ROOT:jet-engine-configuration.adoc[]
- xref:operator:ROOT:jet-job-configuration.adoc[]
- xref:operator:ROOT:jet-job-snapshot.adoc[]
